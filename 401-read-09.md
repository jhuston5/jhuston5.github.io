# Reading 09

## [Dunder Methods](https://dbader.org/blog/python-dunder-methods)

Advice from the reading is to treat "double under" methods like a normal language feature.

When a new empty class is created, a dunder method can be used to add functionality to said class. The author gives examples of the len and splice methods.

Other Dunder Methods that can be used:

- Initialization of new objects
- Object representation
- Enable iteration
- Operator overloading (comparison)
- Operator overloading (addition)
- Method invocation
- Context manager support (with statement)

The first one we are fairly familiar with through our current work with classes - __init__ lets us create objects within the class.

__str__ and __repr__ are two ways to print some extra pieces of data about the objects (representation). The reading indicates that we should default to __repr__.

We can also make the class iterable. The following code defines length and get item methods so that one can actually iterate over the class.

```Python
class Account:
    # ... (see above)

    def __len__(self):
        return len(self._transactions)

    def __getitem__(self, position):
        return self._transactions[position]
```

Operator overloading seems to be adding in methods that might not be inherited by a parent or explicitly stated. Python has a lot of methods running under the hood, and Dunder methods allow us to add some of those in when they might otherwise be excluded.

> A context manager is a simple “protocol” (or interface) that your object needs to follow so it can be used with the with statement. Basically all you need to do is add __enter__ and __exit__ methods to an object if you want it to function as a context manager

## [Statistics - Probability](https://www.dataquest.io/blog/basic-statistics-in-python-probability/)

Probability asks "What is the chance of an event happening?"
We use statistics to calculate probabilities based of sample sets of data.
By providing enough clean, accurate sample data we can get more accurate probabilities with less deviations.

Normal Distribution is our standard probability curve.
Central Limit Theorem:
> With more trials, the closer the average of these trials approach the true probability, even if the individual trials themselves are imperfect.

Essentially, we can come closer to theoretical probability ideals.

Three Sigma Rule: 
>The Three Sigma rule, also known as the empirical rule or 68-95-99.7 rule, is an expression of how many of our observations fall within a certain distance of the mean. Remember that the standard deviation (a.k.a. “sigma”) is the average distance an observation in the data set is from the mean.